{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 単語分散表現\n",
    "\n",
    "本章では、最初に異なるテキストどうしの類似度（近さ）をはかる方法について説明します。単純な BoW （文書単語行列）から紹介し、\n",
    "次に「意味を考慮して形態素をデータ化する方法」と、これを活用してテキストの類似度を調べる方法を説明します。\n",
    "\n",
    "\n",
    "## 文書ベクトル\n",
    "\n",
    "まずベクトルという概念を説明します。\n",
    "\n",
    "たとえば以下では A と Bという2つのリストを用意しています。それぞれの要素は整数ですが、これらがベクトルにあたります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3]\n",
      "[3 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([7,3])\n",
    "B = np.array([3,7])\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "ax = plt.axes()\n",
    "ax.arrow(0.0, 0.0, A[0], A[1], head_width=0.6, head_length=0.5, color = 'red')\n",
    "plt.annotate(f'A({A[0]},{A[1]})', xy=(A[0], A[1]), xytext=(A[0]+0.5, A[1]))\n",
    "ax.arrow(0.0, 0.0, B[0], B[1], head_width=0.4, head_length=0.6, color = 'blue')\n",
    "plt.annotate(f'B({B[0]},{B[1]})', xy=(B[0], B[1]), xytext=(B[0]+0.5, B[1]))\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## コサイン類似度\n",
    "\n",
    "このとき、2つの矢印はそれぞれベクトルを表しています。２つベクトルの近さは、それぞれの矢印の間のコサインで表すことができます。\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{7 \\times 3 + 3 \\times 7 } { \\sqrt{ 7^2 + 3^2} \\times \\sqrt{3^2 + 7^2}} = 0.7241379\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "もっとも、毎回この計算を行うのは大変です。そこで scikit-learn の **cosine_similarity** を使ってみます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "## A の次元数\n",
    "print(A.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "## 次元を変換\n",
    "print(A.reshape(1, -1).ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コサイン類似度:[[0.72413793]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim = cosine_similarity(A.reshape(1, -1), B.reshape(1, -1))\n",
    "print (f'コサイン類似度:{cos_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コサイン類似度:[[0.49957554]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([7, 3, 1, 6, 8])\n",
    "B = np.array([3, 7, 2, 1, 0])\n",
    "cos_sim = cosine_similarity(A.reshape(1, -1), B.reshape(1, -1))\n",
    "print (f'コサイン類似度:{cos_sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## テキストの類似度\n",
    "\n",
    "ところで、文書単語行列では、テキストごとに出現した単語の頻度をベクトルとして表現し、これらを並べて行列として結合したものだと考えることができます。\n",
    "この場合、各文書の次元（ベクトルの要素数）は、すべての文書を通じて出現した語彙数ということになります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ここでも歴代総理大臣所信表明演説データを利用しましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 300)\n"
     ]
    }
   ],
   "source": [
    "## utf8フォルダにあるファイル一覧を取得\n",
    "import os\n",
    "import re\n",
    "files = ['utf8/' + path for path in os.listdir('utf8')]\n",
    "pattern = 'utf8/\\\\d{8}_(\\\\d{1,3}_[a-z]{1,}-[a-z]{1,})_general-policy-speech.txt'\n",
    "results = [re.match(pattern, file_name) for file_name in files]\n",
    "prime_names = [ res.group(1) for res in results]\n",
    "stopwords = [],\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [w.strip() for w in f]\n",
    "## ストップワードをさらに追加\n",
    "stopwords.extend(['あの', 'この', 'ある', 'する', 'いる', 'できる', 'なる', 'れる', 'の', 'は', '〇', 'ソ', 'もつ', 'わが国', 'われわれ','私たち','そのため','行なう','おこなう','%'])\n",
    "## セットに変更（形態素が重複して登録されているのを避けるため）\n",
    "stopwords = set(stopwords)\n",
    "## ストップワードの要素数を確認\n",
    "len(stopwords)\n",
    "## 単語文書行列の作成\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import my_mecab_stopwords as my_tokenizer\n",
    "args={'stopwords_list': stopwords}\n",
    "vectorizer = CountVectorizer(input='filename', lowercase=False,\n",
    "                             max_df=0.5, max_features=300,\n",
    "                             tokenizer=lambda text: my_tokenizer.tokens(text, **args))\n",
    "prime_dtm = vectorizer.fit_transform(files)\n",
    "## 文書単語行列のサイズを確認\n",
    "print(prime_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "テキスト分析では、文書の長さを合わせることは必ずしも簡単ではありません。そこで、文書の長さに合わせて頻度を調整することを考えます。それが先の章で紹介した TF-IDF ということになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "## 各ドキュメントのTF-IDFを計算\n",
    "tfidf = transformer.fit_transform(prime_dtm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>あす</th>\n",
       "      <th>あわせる</th>\n",
       "      <th>いう</th>\n",
       "      <th>かかわる</th>\n",
       "      <th>ここに</th>\n",
       "      <th>つくる</th>\n",
       "      <th>とる</th>\n",
       "      <th>とれる</th>\n",
       "      <th>はかる</th>\n",
       "      <th>ふさわしい</th>\n",
       "      <th>...</th>\n",
       "      <th>関心</th>\n",
       "      <th>関連</th>\n",
       "      <th>関連法案</th>\n",
       "      <th>防止</th>\n",
       "      <th>需要</th>\n",
       "      <th>首脳</th>\n",
       "      <th>高い</th>\n",
       "      <th>高める</th>\n",
       "      <th>高度</th>\n",
       "      <th>ＩＴ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47_sato-eisaku</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039883</td>\n",
       "      <td>0.125580</td>\n",
       "      <td>0.432913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039883</td>\n",
       "      <td>0.039263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077324</td>\n",
       "      <td>0.125580</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185_abe-shinzo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151465</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040371</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26_kishi-nobusuke</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163_koizumi-jyunichiro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157796</td>\n",
       "      <td>0.132847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49_sato-eisaku</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055725</td>\n",
       "      <td>0.188389</td>\n",
       "      <td>0.053092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055725</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81_fukuda-takeo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059124</td>\n",
       "      <td>0.060965</td>\n",
       "      <td>0.064998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185782</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>0.167190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127_hosokawa-morihiro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074184</td>\n",
       "      <td>0.038931</td>\n",
       "      <td>0.040262</td>\n",
       "      <td>0.037686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066760</td>\n",
       "      <td>0.107870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119_kaifu-toshiki</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033266</td>\n",
       "      <td>0.064522</td>\n",
       "      <td>0.033266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029975</td>\n",
       "      <td>0.067581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033791</td>\n",
       "      <td>0.121637</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149_mori-yoshiro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025784</td>\n",
       "      <td>0.026587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>0.079761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026180</td>\n",
       "      <td>0.028345</td>\n",
       "      <td>0.497857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150_mori-yoshiro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040079</td>\n",
       "      <td>0.024178</td>\n",
       "      <td>0.020356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.054956</td>\n",
       "      <td>0.059198</td>\n",
       "      <td>0.021365</td>\n",
       "      <td>0.579933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         あす      あわせる        いう      かかわる       ここに       つくる  \\\n",
       "47_sato-eisaku          0.0  0.078526  0.000000  0.000000  0.000000  0.000000   \n",
       "185_abe-shinzo          0.0  0.151465  0.036723  0.000000  0.000000  0.170602   \n",
       "26_kishi-nobusuke       0.0  0.000000  0.000000  0.000000  0.409729  0.000000   \n",
       "163_koizumi-jyunichiro  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "49_sato-eisaku          0.0  0.052268  0.000000  0.000000  0.055725  0.188389   \n",
       "...                     ...       ...       ...       ...       ...       ...   \n",
       "81_fukuda-takeo         0.0  0.000000  0.059124  0.060965  0.064998  0.000000   \n",
       "127_hosokawa-morihiro   0.0  0.000000  0.000000  0.000000  0.038931  0.000000   \n",
       "119_kaifu-toshiki       0.0  0.033266  0.064522  0.033266  0.000000  0.029975   \n",
       "149_mori-yoshiro        0.0  0.000000  0.025784  0.026587  0.000000  0.047914   \n",
       "150_mori-yoshiro        0.0  0.000000  0.019434  0.000000  0.000000  0.054171   \n",
       "\n",
       "                              とる       とれる       はかる     ふさわしい  ...        関心  \\\n",
       "47_sato-eisaku          0.039883  0.125580  0.432913  0.000000  ...  0.039883   \n",
       "185_abe-shinzo          0.000000  0.040371  0.000000  0.000000  ...  0.000000   \n",
       "26_kishi-nobusuke       0.000000  0.000000  0.211870  0.000000  ...  0.000000   \n",
       "163_koizumi-jyunichiro  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "49_sato-eisaku          0.053092  0.000000  0.230521  0.000000  ...  0.000000   \n",
       "...                          ...       ...       ...       ...  ...       ...   \n",
       "81_fukuda-takeo         0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "127_hosokawa-morihiro   0.074184  0.038931  0.040262  0.037686  ...  0.037092   \n",
       "119_kaifu-toshiki       0.067581  0.000000  0.000000  0.034332  ...  0.000000   \n",
       "149_mori-yoshiro        0.000000  0.028345  0.000000  0.109755  ...  0.027006   \n",
       "150_mori-yoshiro        0.000000  0.000000  0.000000  0.103407  ...  0.000000   \n",
       "\n",
       "                              関連      関連法案        防止        需要        首脳  \\\n",
       "47_sato-eisaku          0.039263  0.000000  0.039883  0.000000  0.071783   \n",
       "185_abe-shinzo          0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "26_kishi-nobusuke       0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "163_koizumi-jyunichiro  0.000000  0.157796  0.132847  0.000000  0.000000   \n",
       "49_sato-eisaku          0.000000  0.000000  0.000000  0.053092  0.000000   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "81_fukuda-takeo         0.000000  0.000000  0.000000  0.185782  0.055730   \n",
       "127_hosokawa-morihiro   0.000000  0.000000  0.037092  0.000000  0.000000   \n",
       "119_kaifu-toshiki       0.000000  0.000000  0.000000  0.033791  0.121637   \n",
       "149_mori-yoshiro        0.079761  0.000000  0.027006  0.000000  0.145823   \n",
       "150_mori-yoshiro        0.040079  0.024178  0.020356  0.000000  0.018319   \n",
       "\n",
       "                              高い       高める        高度        ＩＴ  \n",
       "47_sato-eisaku          0.000000  0.077324  0.125580  0.000000  \n",
       "185_abe-shinzo          0.103844  0.000000  0.040371  0.000000  \n",
       "26_kishi-nobusuke       0.000000  0.189213  0.000000  0.000000  \n",
       "163_koizumi-jyunichiro  0.000000  0.000000  0.000000  0.000000  \n",
       "49_sato-eisaku          0.000000  0.000000  0.055725  0.000000  \n",
       "...                          ...       ...       ...       ...  \n",
       "81_fukuda-takeo         0.167190  0.000000  0.000000  0.000000  \n",
       "127_hosokawa-morihiro   0.066760  0.107870  0.000000  0.000000  \n",
       "119_kaifu-toshiki       0.030409  0.000000  0.000000  0.000000  \n",
       "149_mori-yoshiro        0.000000  0.026180  0.028345  0.497857  \n",
       "150_mori-yoshiro        0.054956  0.059198  0.021365  0.579933  \n",
       "\n",
       "[82 rows x 300 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tfidf.todense(), columns = vectorizer.get_feature_names(), index = prime_names )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "この文書単語行列の特徴は、文書ごとに長さが1に正規化されていることです。ここで **正規化** とは、各文書の要素をそれぞれ自乗して合計した値の平方根を取ると、1 になっているということです。検算してみましょう。ちなみに、平方根をとるというのは、0.5 乗するということです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47_sato-eisaku            1.0\n",
      "185_abe-shinzo            1.0\n",
      "26_kishi-nobusuke         1.0\n",
      "163_koizumi-jyunichiro    1.0\n",
      "49_sato-eisaku            1.0\n",
      "                         ... \n",
      "81_fukuda-takeo           1.0\n",
      "127_hosokawa-morihiro     1.0\n",
      "119_kaifu-toshiki         1.0\n",
      "149_mori-yoshiro          1.0\n",
      "150_mori-yoshiro          1.0\n",
      "Length: 82, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.apply(lambda x: (x**2).sum()** 0.5, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "確かに正規化されています。所信表明演説のすべてのペアについて類似度を測ります。これは簡単です。\n",
    "文章単語行列全体に `cosine_similarity()` を適用するだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['47_sato-eisaku', '185_abe-shinzo']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.09501386, 0.24995617, ..., 0.22591215, 0.18308799,\n",
       "        0.14639476],\n",
       "       [0.09501386, 1.        , 0.03855714, ..., 0.25610042, 0.20345656,\n",
       "        0.18659045],\n",
       "       [0.24995617, 0.03855714, 1.        , ..., 0.10611083, 0.08016128,\n",
       "        0.08328839],\n",
       "       ...,\n",
       "       [0.22591215, 0.25610042, 0.10611083, ..., 1.        , 0.25651426,\n",
       "        0.23089066],\n",
       "       [0.18308799, 0.20345656, 0.08016128, ..., 0.25651426, 1.        ,\n",
       "        0.72633377],\n",
       "       [0.14639476, 0.18659045, 0.08328839, ..., 0.23089066, 0.72633377,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prime_sim = cosine_similarity(df)\n",
    "prime_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "例えば1行目の出力の見方は、0番目の所信表明演説(47_sato-eisaku) と 0 番目の所信表明演説の類似度は 1 ということです。つまり、完全に一致しているということになります。これは当然ですね。その横にある 0.30063538 は0番目の所信表明演説と1番目の所信表明演説(185_abe-shinzo)の類似度です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "各行ごとに最大値、つまりもっとも類似している所信表明演説を確認してみましょう。なお、各行の最大値は 1 になっています。これは、自分自身との類似度を求めているので、当然です。\n",
    "ちなみに、自分自身との類似度は行列の対角要素になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 行列の対角成分を出力する\n",
    "np.diag(prime_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "単純に各行の最大値を求めてしまうと、自分自身との類似度1が選ばれてしまいます。そこで、この対角要素を `np.fill_diagonal()` で0 に変えてから、最大値を求めることにします。\n",
    "3行3列の行列を例に、この作業のイメージを示しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "行列の対角成分を0に変える\n",
      "[[0 2 3]\n",
      " [4 0 6]\n",
      " [7 8 0]]\n"
     ]
    }
   ],
   "source": [
    "## 単純な行列を作成し\n",
    "A = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "print(A)\n",
    "print('行列の対角成分を0に変える')\n",
    "np.fill_diagonal(A, 0)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "では、文書単語行列の対角成分を0に置き換えてしまいます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.09501386, 0.24995617, ..., 0.22591215, 0.18308799,\n",
       "        0.14639476],\n",
       "       [0.09501386, 0.        , 0.03855714, ..., 0.25610042, 0.20345656,\n",
       "        0.18659045],\n",
       "       [0.24995617, 0.03855714, 0.        , ..., 0.10611083, 0.08016128,\n",
       "        0.08328839],\n",
       "       ...,\n",
       "       [0.22591215, 0.25610042, 0.10611083, ..., 0.        , 0.25651426,\n",
       "        0.23089066],\n",
       "       [0.18308799, 0.20345656, 0.08016128, ..., 0.25651426, 0.        ,\n",
       "        0.72633377],\n",
       "       [0.14639476, 0.18659045, 0.08328839, ..., 0.23089066, 0.72633377,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.fill_diagonal(prime_sim, 0)\n",
    "prime_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "対角成分を確認してみましょう。1行目と1列目、また2行目と2列目の成分を表示してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(prime_sim[0][0])\n",
    "print(prime_sim[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31 51 71 58 57 75 31 31 45 21 36 16 64 65 39 59 78 22 47 60 53  9 58 30\n",
      " 75 41  9 31 48 28 23 75 24 62  0 49 10 38 64 14 65 25 65 17 38  8 52 53\n",
      " 28 35 80  1 49 56 58 30 53  4 22 56 24 78 61 45 38 42 68 73 28 36  0 32\n",
      " 31 67 62 31  9 37 61 74 81 80]\n"
     ]
    }
   ],
   "source": [
    "row_max_index = np.argmax(prime_sim, axis=1)\n",
    "print(row_max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "対角要素と、その右上の要素をすべて0に変えます。以下にこの作業のイメージを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 3]\n",
      " [2 0 1]\n",
      " [3 1 0]]\n",
      "右上の要素をすべて0に変える\n",
      "[[0 0 0]\n",
      " [2 0 0]\n",
      " [3 1 0]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0,2,3], [2,0,1], [3,1,0]])\n",
    "print(A)\n",
    "print('右上の要素をすべて0に変える')\n",
    "A = np.tril(A)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "後の作業のため、ここで所信表明演説の類似度行列のコピーを作成し、このコピーを操作します。いったんデータフレームに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "row number = 39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>57_sato-eisaku</td>\n",
       "      <td>62_sato-eisaku</td>\n",
       "      <td>0.875145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59_sato-eisaku</td>\n",
       "      <td>57_sato-eisaku</td>\n",
       "      <td>0.808150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>127_hosokawa-morihiro</td>\n",
       "      <td>128_hosokawa-morihiro</td>\n",
       "      <td>0.743670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>178_noda-yoshihiko</td>\n",
       "      <td>179_noda-yoshihiko</td>\n",
       "      <td>0.731338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>150_mori-yoshiro</td>\n",
       "      <td>149_mori-yoshiro</td>\n",
       "      <td>0.726334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>144_obuchi-keizo</td>\n",
       "      <td>143_obuchi-keizo</td>\n",
       "      <td>0.696456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>168_fukuda-yasuo</td>\n",
       "      <td>168_abe-shinzo</td>\n",
       "      <td>0.686183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>151_koizumi-jyunichiro</td>\n",
       "      <td>153_koizumi-jyunichiro</td>\n",
       "      <td>0.677783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>88_ohira-masayoshi</td>\n",
       "      <td>90_ohira-masayoshi</td>\n",
       "      <td>0.672031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>41_ikeda-hayato</td>\n",
       "      <td>44_ikeda-hayato</td>\n",
       "      <td>0.660358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>187_abe-shinzo</td>\n",
       "      <td>185_abe-shinzo</td>\n",
       "      <td>0.654346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50_sato-eisaku</td>\n",
       "      <td>49_sato-eisaku</td>\n",
       "      <td>0.651360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141_hashimoto-ryutaro</td>\n",
       "      <td>139_hashimoto-ryutaro</td>\n",
       "      <td>0.631275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>125_miyazawa-kiichi</td>\n",
       "      <td>128_hosokawa-morihiro</td>\n",
       "      <td>0.618471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107_nakasone-yasuhiro</td>\n",
       "      <td>103_nakasone-yasuhiro</td>\n",
       "      <td>0.616794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>131_murayama-tomiichi</td>\n",
       "      <td>134_murayama-tomiichi</td>\n",
       "      <td>0.612548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>74_miki-takeo</td>\n",
       "      <td>76_miki-takeo</td>\n",
       "      <td>0.603419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>111_takeshita-noboru</td>\n",
       "      <td>113_takeshita-noboru</td>\n",
       "      <td>0.598805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97_nakasone-yasuhiro</td>\n",
       "      <td>100_nakasone-yasuhiro</td>\n",
       "      <td>0.593987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>129_hata-tsutomu</td>\n",
       "      <td>131_murayama-tomiichi</td>\n",
       "      <td>0.586209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         X                       Y  similarity\n",
       "17          57_sato-eisaku          62_sato-eisaku    0.875145\n",
       "20          59_sato-eisaku          57_sato-eisaku    0.808150\n",
       "36   127_hosokawa-morihiro   128_hosokawa-morihiro    0.743670\n",
       "25      178_noda-yoshihiko      179_noda-yoshihiko    0.731338\n",
       "38        150_mori-yoshiro        149_mori-yoshiro    0.726334\n",
       "8         144_obuchi-keizo        143_obuchi-keizo    0.696456\n",
       "31        168_fukuda-yasuo          168_abe-shinzo    0.686183\n",
       "19  151_koizumi-jyunichiro  153_koizumi-jyunichiro    0.677783\n",
       "11      88_ohira-masayoshi      90_ohira-masayoshi    0.672031\n",
       "33         41_ikeda-hayato         44_ikeda-hayato    0.660358\n",
       "14          187_abe-shinzo          185_abe-shinzo    0.654346\n",
       "18          50_sato-eisaku          49_sato-eisaku    0.651360\n",
       "3    141_hashimoto-ryutaro   139_hashimoto-ryutaro    0.631275\n",
       "22     125_miyazawa-kiichi   128_hosokawa-morihiro    0.618471\n",
       "6    107_nakasone-yasuhiro   103_nakasone-yasuhiro    0.616794\n",
       "13   131_murayama-tomiichi   134_murayama-tomiichi    0.612548\n",
       "24           74_miki-takeo           76_miki-takeo    0.603419\n",
       "7     111_takeshita-noboru    113_takeshita-noboru    0.598805\n",
       "0     97_nakasone-yasuhiro   100_nakasone-yasuhiro    0.593987\n",
       "15        129_hata-tsutomu   131_murayama-tomiichi    0.586209"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(prime_sim))\n",
    "prime_sim2 =  np.tril(prime_sim)\n",
    "df2 = pd.DataFrame(columns=['X', 'Y', 'similarity'])\n",
    "\n",
    "for i, j in enumerate(row_max_index):\n",
    "    if prime_sim2[i][j] == 0:\n",
    "        continue\n",
    "    else:\n",
    "        df2 = df2.append({'X': prime_names[i] , 'Y': prime_names[j], 'similarity': prime_sim2[i][j] }, ignore_index=True)\n",
    "df2.sort_values('similarity', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ペアの類似度が高いのは、ほとんどが同じ総理大臣による（別の国会での別の）演説となっています。これは互いに内容が似ているのも当然でしょう。また、異なる総理大臣による所信表明演説が高い類似度を示している場合は、時代間隔（国会の開催年月日）がきわめて近いことも確認できます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "たとえば、安倍晋三氏は 168 期の国会で退陣し、その 5 年後に再び総理大臣に返り咲いています。つまり最初の就任期と、第183回との間には時間的なズレがあり、そのため演説の内容やスタンスも変わっていると思われます。確認してみましょう。最初に作成した `prime_sim` から検索します。 ただし、この配列には行名（また列名）がないので、いったんデータフレームに変換しましょう。この結果を `filter()` を使って絞り込みます。まず index に `abe` が含まれている行を抽出し、続けて列名に `abe` が含まれている列を探します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>185_abe-shinzo</th>\n",
       "      <th>183_abe-shinzo</th>\n",
       "      <th>165_abe-shinzo</th>\n",
       "      <th>187_abe-shinzo</th>\n",
       "      <th>168_abe-shinzo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165_abe-shinzo</th>\n",
       "      <td>0.325235</td>\n",
       "      <td>0.317965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.251582</td>\n",
       "      <td>0.494684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168_abe-shinzo</th>\n",
       "      <td>0.276883</td>\n",
       "      <td>0.239228</td>\n",
       "      <td>0.494684</td>\n",
       "      <td>0.226264</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183_abe-shinzo</th>\n",
       "      <td>0.511128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.317965</td>\n",
       "      <td>0.352420</td>\n",
       "      <td>0.239228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185_abe-shinzo</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.511128</td>\n",
       "      <td>0.325235</td>\n",
       "      <td>0.682125</td>\n",
       "      <td>0.276883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187_abe-shinzo</th>\n",
       "      <td>0.682125</td>\n",
       "      <td>0.352420</td>\n",
       "      <td>0.251582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.226264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                185_abe-shinzo  183_abe-shinzo  165_abe-shinzo  \\\n",
       "165_abe-shinzo        0.325235        0.317965        1.000000   \n",
       "168_abe-shinzo        0.276883        0.239228        0.494684   \n",
       "183_abe-shinzo        0.511128        1.000000        0.317965   \n",
       "185_abe-shinzo        1.000000        0.511128        0.325235   \n",
       "187_abe-shinzo        0.682125        0.352420        0.251582   \n",
       "\n",
       "                187_abe-shinzo  168_abe-shinzo  \n",
       "165_abe-shinzo        0.251582        0.494684  \n",
       "168_abe-shinzo        0.226264        1.000000  \n",
       "183_abe-shinzo        0.352420        0.239228  \n",
       "185_abe-shinzo        0.682125        0.276883  \n",
       "187_abe-shinzo        1.000000        0.226264  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame(prime_sim, columns=prime_names, index=prime_names)\n",
    "df3.filter(like = 'abe', axis=0).filter(regex='abe').sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "出力が分かりにくいかと思いますが、例えば1行目 (165_abe-shinzo) は第165回国会での所信表明演説にあたり、その内容について第1列目(165_abe-shinzo)、すなわち第185回演説との近さを測ると約 32％となっています。一方、3列めの第165回演説との類似度は1ということです。つまり、完全に一致しているということになります。これは当然ですね。その横にある 0.51687549 は1番目の所信表明演説と2番目の所信表明演説(168_abe-shinzo )の類似度です。ちなみに安倍元総理は、この段階で退陣し、5年後に再び総理大臣に返り咲いています。つまり最初の就任期と、第183回との間には時間的なズレがあり、そのため演説の内容やスタンスも変わっていると思われます。実際、時間的に離れた所信表明演説の類似度は小さくなっているようです。\n",
    "\n",
    "165 期と 168 期の演説の類似度は約 0.495 です。また 183 期以降の演説それぞれの類似度はそれぞれ 0.5 を超えています。が、165 期ないし 168 期の演説と、183 期以降の演説との類似度は 0.2 から 0.3 のあいだとなっています。\n",
    "\n",
    "一方、181 期の旧民主党の野田元総理と 185 期の自民党安倍元総理の類似度が 0.67 と高いことに気が付きます。\n",
    "参考までに文書単語行列に登録された単語の一覧をみてみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['%', 'あわせる', 'つくる', 'とる', 'はかる', 'ふさわしい', 'めぐる', 'もたらす', 'よい', 'エネルギー', 'サミット', 'システム', '一人一人', '一体', '上げる', '上昇', '不可欠', '世代', '世界経済', '事件', '事態', '事業', '交流', '人間', '人類', '住宅', '住民', '供給', '価格', '克服', '公務員', '公正', '内閣総理大臣', '円滑', '再生', '削減', '前進', '割り', '創造', '力強い', '効果', '動き', '動向', '医療', '協力関係', '協議', '危機', '参加', '収支', '取りまとめる', '取り戻す', '取り組み', '受けとめる', '合う', '合意', '向かう', '含む', '含める', '国交', '国内', '国土', '国政', '国連', '国際的', '土地', '均衡', '型', '基礎', '堅持', '大切', '大統領', '姿', '姿勢', '存じる', '安全保障', '安心', '寄与', '対話', '展望', '展開', '幅広い', '平成', '年金', '年間', '引き続く', '形成', '役割', '従来', '復興', '徹底', '応じる', '思い', '意味', '懸案', '戦略', '所存', '払う', '技術', '投資', '担う', '拡充', '提案', '支える', '改正', '政治家', '政治改革', '方向', '日本人', '日本経済', '昭和', '是正', '最大', '最大限', '未来', '案', '構築', '構造', '構造改革', '次第', '正常化', '歳出', '歴史', '民主主義', '民間', '水準', '沖', '沖縄', '活動', '活性化', '消費者', '深刻', '物価', '率直', '現実', '理念', '生かす', '生じる', '発揮', '皆さん', '皆様', '真', '真剣', '石油', '確信', '科学技術', '秩序', '税制', '策', '策定', '経済成長', '経済的', '続く', '緊急', '締結', '編成', '緩和', '繩', '置く', '考え方', '育成', '至る', '行動', '行政改革', '被災地', '補正予算', '見直す', '規制', '言う', '訴える', '調和', '調整', '諸君', '諸問題', '講ずる', '議員', '議論', '貿易', '資源', '質', '超える', '転換', '輸入', '農業', '速やか', '連携', '進む', '進展', '遂げる', '過去', '配慮', '重ねる', '重大', '長期', '関連', '防止', '需要', '首脳', '高い', '高める', '高度']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "安倍総理の演説における「復興」と「被災地」の出現確率を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>復興</th>\n",
       "      <th>被災地</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185_abe-shinzo</th>\n",
       "      <td>0.214127</td>\n",
       "      <td>0.223403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183_abe-shinzo</th>\n",
       "      <td>0.239159</td>\n",
       "      <td>0.332693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165_abe-shinzo</th>\n",
       "      <td>0.045612</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187_abe-shinzo</th>\n",
       "      <td>0.124784</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168_abe-shinzo</th>\n",
       "      <td>0.053980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      復興       被災地\n",
       "185_abe-shinzo  0.214127  0.223403\n",
       "183_abe-shinzo  0.239159  0.332693\n",
       "165_abe-shinzo  0.045612  0.000000\n",
       "187_abe-shinzo  0.124784  0.000000\n",
       "168_abe-shinzo  0.053980  0.000000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(like='abe', axis=0)[['復興', '被災地']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>復興</th>\n",
       "      <th>被災地</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181_noda-yoshihiko</th>\n",
       "      <td>0.151557</td>\n",
       "      <td>0.337328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179_noda-yoshihiko</th>\n",
       "      <td>0.269989</td>\n",
       "      <td>0.657266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178_noda-yoshihiko</th>\n",
       "      <td>0.296566</td>\n",
       "      <td>0.412551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          復興       被災地\n",
       "181_noda-yoshihiko  0.151557  0.337328\n",
       "179_noda-yoshihiko  0.269989  0.657266\n",
       "178_noda-yoshihiko  0.296566  0.412551"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(like='noda', axis=0)[['復興', '被災地']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "## 単語分散表現\n",
    "\n",
    "前章までは、テキストから BoW を作成していました。BoW をベースにしたモデルでは、文書ごとに形態素（単語）の出現回数を調べ、必要に応じて頻度を TF-IDF などに変換したデータを分析の出発点とします。\n",
    "この場合、形態素が出現した位置、あるいは文脈（その形態素が、別のどのような形態素の後ないし前に出現したのか）は考慮されていません。\n",
    "\n",
    "しかしながら、文章においては、形態素が文中のどこに出現しているかは非常に重要です。ある形態素がどの位置に出現するかは、まず文章全体によって決まります。つまり、出現位置は、文章の意味と深く関わっています。形態素の多くは特定の文脈に現れることから、同じような文脈に表れる形態素の意味は似ているとする仮説があります。たとえば以下の2つの文は、同じような文脈を表しています。\n",
    "\n",
    "1. 校庭 で サッカー を した。\n",
    "2. 校庭 で 野球 を した。\n",
    "\n",
    "サッカーと野球は似て非なるもののですが、ボールを使う球技で人間が「する」ものであり、かつ校庭にような広いスペースを必要とするという共通点があります。\n",
    "出現する文脈が似ている場合、その意味も似ていると考えるのが、**分布仮説** です。そこで、大量のテキストから、ある形態素がどのような文脈で出現したかを調べれば、分布の似ている形態素を知ることができます。そこで、ある形態素の意味を、近隣に出現しやすい形態素との近さとして表現できると便利です。\n",
    "\n",
    "\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "**単語分散表現** の嚆矢である Word2Vec では、大規模なテキスト集合であるコーパスから単語分散表現を作成します。\n",
    "この際には主に2つの方法が使われています。\n",
    "1つは、ある特定の文脈に出現しやすい単語を予測できるようなモデルを生成することです。\n",
    "もう1つは、ある単語の周辺に出現しやすい単語群を予測できるようなモデルを生成することになります。\n",
    "前者を CBOW, 後者を **skip-gram** と呼びます。 \n",
    "\n",
    "いずれも方法でも、入力となるのは単語の **ワンホットベクトル** です。\n",
    "\n",
    "\n",
    "### ワンホットベクトル\n",
    "\n",
    "いま、出現する語が「犬」「猫」「猿」「雉」「人」の5つだけのテキストがあるとします。\n",
    "このとき、「犬」を次のベクトルで表現します。このとき、それぞれの単語を次のベクトルで表現します。行列の列（縦）が各単語のベクトルになります。\n",
    "\n",
    "| 犬 | 猫 | 猿 | 雉 | 人 | \n",
    "|---|---|---|--|---|\n",
    "| 1 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 1 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "要は、行ごとにみると1行名が「犬要素」、2行目が「猫要素」、3行目が「猿要素」、4行目が「雉要素」、5行目が「人要素」に対応しているわけです。\n",
    "そして1列目の犬は、5行のうち該当するのが1行目だけであり、ここを 1 とし、ほかがすべて0になります。\n",
    "単純ですが、これにより各単語が 5 次元のベクトルで表され、互いに区別できるわけです。\n",
    "\n",
    "ただし、読者の中には、猿と人は近いから、猿ベクトルと人ベクトルは、「猿要素」と「犬要素」の両方に反応しても良いのではないかと思う方も要るかもしれません。\n",
    "\n",
    "| 犬 | 猫 | 猿 | 雉 | 人 | \n",
    "|---|---|---|--|---|\n",
    "| 1 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 0 | 1 |\n",
    "| 0 | 0 | 0 | 1 | 0 |\n",
    "| 0 | 0 | 1 | 0 | 1 |\n",
    "\n",
    "ただ、こうすると「猿」と「人」のベクトルがまったく同じなって区別がつきません。\n",
    "そこで、たとえば次のようにして、「人」は「人要素」が強く、「猿要素」は弱いが、「猿」は逆であることを示す頃ができるかもしれません。\n",
    "\n",
    "\n",
    "| 犬 | 猫 | 猿 | 雉 | 人 | \n",
    "|---|---|---|--|---|\n",
    "| 1 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0.8 | 0 | 0.1 |\n",
    "| 0 | 0 | 0 | 1 | 0 |\n",
    "| 0 | 0 | 0.2 | 0 | 0.9 |\n",
    "\n",
    "単語を複数次元のベクトルで表すということは、このように単語と単語の違いに加えて、その近さ（類似度）を表現することでもあります。\n",
    "上で猿ベクトルと人ベクトルの数値は適当に決めましたが、これらを大規模テキストデータを使って計算しようというのが、ここで紹介するワードベクトルになります。\n",
    "\n",
    "\n",
    "### skip-gram\n",
    "\n",
    "東北大学の乾・岡崎研究室[^tohoku_uni] では、研究過程で作成した単語分散表現を公開しています。\n",
    "以下、本書でも、このモデルを利用させてもらいます。\n",
    "\n",
    "[^tohoku_uni]: <https://www.nlp.ecei.tohoku.ac.jp/>\n",
    "\n",
    "\n",
    "ダウンロードした単語分散表現を扱うために Python に **gensim** ライブラリを導入します。\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "準備が整いましたので、ダウンロードした単語分散表現を使ってみましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/2bddf92b-47f9-4809-95a5-b91e7f25af27/myData/GitHub/textmining_python/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "## ダウンロードした単語分散表現が Jupyter を起動しているフォルダにあるとします\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('entity_vector.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ここで利用するのモデルはファイル名の末尾が bin となっています\n",
    "これを使って、「徳島」がここでどのように表現されているかを確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1451146   0.64631635  1.1206466   0.53557366  1.1265059  -0.47855055\n",
      " -1.9572403  -0.36092016 -0.12230052  0.00528774 -0.9410112   0.5655498\n",
      " -0.34019628  0.03107905  1.2963295  -0.03178046  0.67060477  1.1305983\n",
      " -0.3368416  -1.2133294   1.1458337  -1.2171845   1.3975791  -0.8819545\n",
      "  0.8784004  -1.0548002  -1.2283356   0.04045669 -0.01702263 -0.32386667\n",
      " -1.0599047   0.8248806   0.12551713 -0.12358826  2.4171948   0.5412358\n",
      "  1.6744537   0.7213261  -0.07311266  1.221979    0.60591567  0.6359788\n",
      " -1.6467801  -0.483416    0.33695164  0.52571183 -0.8949706   0.6989222\n",
      "  0.10594787  1.4380262   2.4102638  -1.5664365  -0.1476826  -1.0740054\n",
      "  0.13817035  0.28903145  1.1353608   0.02972492  0.42153484  0.3525321\n",
      "  0.41672072  0.45877242 -1.367437   -1.2573254   0.44229034  1.6489668\n",
      " -0.5580291   1.5877541   0.45503724  0.41794908 -0.48812628 -0.75694937\n",
      "  0.16592807  0.21474564 -0.39681706  0.40212667 -0.72284573 -1.7669711\n",
      " -0.71841913 -0.3941651   1.3607436   1.4318576   0.43657574 -1.0812192\n",
      " -1.4796697   0.8808305  -2.896217   -0.28217202 -0.10230227 -1.8142314\n",
      " -0.7627604   0.61423904 -0.407716    0.9422217  -1.2616667   2.3023157\n",
      " -2.4974165   0.33682728  0.31226307 -1.6989379  -0.612968    0.9000666\n",
      " -0.79090774 -0.42063704  0.19587651 -0.02609681 -0.40632007 -0.14033686\n",
      " -0.30916756  0.0440501   0.26087508 -0.5646983  -1.0106723  -1.3726658\n",
      " -0.24852811 -1.0716788  -0.04256602 -0.15830214 -0.14624959  1.3771601\n",
      "  0.78676933  1.7137731  -0.7104532   0.01694393  0.698215    0.20814341\n",
      " -0.47401345 -1.3165212  -0.30451518 -0.33904967  0.67839813  0.07755331\n",
      " -0.07964803 -1.2001079  -1.3295679  -0.20558964  1.2172337   0.9167497\n",
      "  0.24913028  1.266321   -0.71333826  2.1852624  -0.07494659 -1.30173\n",
      " -0.22887936  0.35512725  0.7201695   0.53420866  0.18403317  2.0518787\n",
      " -0.6226907  -0.37034342  0.2113158  -1.022931   -1.2034961  -1.6787721\n",
      "  0.34630585 -2.0976436   0.30796668 -0.29899076 -0.1294589  -0.34831455\n",
      "  0.2448933  -0.312876    0.5916982  -1.3266097   0.67156494  0.8034996\n",
      "  0.5298861   0.95500755  1.0610118   1.1583117   0.40923405 -0.23056316\n",
      " -1.4721186   2.5312393  -0.24470648  0.7045202   1.8048768   0.01326977\n",
      "  2.409381    1.6333113  -0.98599666 -0.17802572  0.03622497 -0.16439998\n",
      "  0.11351351 -1.5526029  -0.4652032   0.72413975 -1.8166566   1.4446893\n",
      " -1.3221014   0.9393928   1.338765    0.40931824 -0.23101576 -3.3050535\n",
      "  0.58510953  0.02195218]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model['徳島'])\n",
    "model['徳島'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('香川県', 0.8651489615440369),\n",
       " ('岡山県', 0.8528105616569519),\n",
       " ('山口県', 0.8483657240867615),\n",
       " ('愛媛県', 0.8469836711883545),\n",
       " ('高知県', 0.8455371856689453),\n",
       " ('熊本県', 0.8427625894546509),\n",
       " ('新潟県', 0.8358582258224487),\n",
       " ('島根県', 0.8284846544265747),\n",
       " ('鳥取県', 0.8260581493377686),\n",
       " ('静岡県', 0.8161298632621765)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('徳島県')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "上位 5 つは、要するに中国・四国地方に位置する県になっています。他に、熊本や新潟が似ていると判断されています。\n",
    "\n",
    "もう少し、単語分散表現で遊んでみましょう。単語分散表現で話題によく上がるのが、概念を計算できることです。「東京」から「日本」を引いて、「フランス」を足すという操作ができるのです。\n",
    "\n",
    "`東京 - 日本 + フランス`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('パリ', 0.7462971210479736),\n",
       " ('[パリ]', 0.6993756294250488),\n",
       " ('ベルリン', 0.6419284343719482),\n",
       " ('ロンドン', 0.6390188336372375),\n",
       " ('ミラノ', 0.6374871730804443),\n",
       " ('ウィーン', 0.6211003065109253),\n",
       " ('ブリュッセル', 0.6124843955039978),\n",
       " ('ミュンヘン', 0.6093114614486694),\n",
       " ('ハンブルク', 0.5993486642837524),\n",
       " ('[リヨン]', 0.5960404872894287)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['東京', 'フランス'], negative=['日本'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "次に、「阿波おどり」から「徳島」を引いて、高知を足してみましょう。\n",
    "\n",
    "`阿波おどり - 徳島 + 高知`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[よさこい]', 0.6300591230392456),\n",
       " ('夏祭り', 0.5732635259628296),\n",
       " ('[よさこい祭り]', 0.5656487941741943),\n",
       " ('[YOSAKOI]', 0.5619512796401978),\n",
       " ('夏まつり', 0.5619202256202698),\n",
       " ('総踊り', 0.5596013069152832),\n",
       " ('夏祭', 0.5583030581474304),\n",
       " ('[阿波踊り]', 0.5561996102333069),\n",
       " ('おどり', 0.548295259475708),\n",
       " ('まつり', 0.5472521185874939)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['阿波おどり', '高知'], negative=['徳島'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "「よさこい」がもっともらしいと出ました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['徳島', '阿波', '踊り', '見学', 'する']\n",
      "['青森', 'ねぶた', '祭', '観る']\n",
      "['福岡', '豚', '骨', 'ラーメン', '食べる']\n"
     ]
    }
   ],
   "source": [
    "import my_janome_stopwords as jnm\n",
    "text1 = jnm.tokens('徳島で、阿波踊りを見学した')\n",
    "print(text1)\n",
    "text2 = jnm.tokens('青森で、ねぶた祭を観た')\n",
    "print(text2)\n",
    "text3 = jnm.tokens('福岡で、豚骨ラーメンを食べた')\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "それぞれの文章から抽出された形態素は、いずれもが 200 次元のベクトルで表現されています。\n",
    "\n",
    "\n",
    "ある文章が3つの形態素からなっていたとします。この形態素をそれぞれ word1,word2,word3 とします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "word1 = np.array([1,2,3,4,5])\n",
    "word2 = np.array([5,4,3,2,1])\n",
    "word3 = np.array([1,0,2,0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.33333333, 2.        , 2.66666667, 2.        , 3.        ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word1 + word2 + word3\n",
    "words / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "つまり、この3語からなる文章を 5 次元のベクトルに表現することができました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avg_vec(word_list, model):\n",
    "    ## 計算結果を蓄積する空のベクトルを用意しておく\n",
    "    vec_ = np.zeros((200,), dtype='float32') \n",
    "    for word in word_list:\n",
    "        vec_ = np.add(vec_, model[word])\n",
    "    if len(word_list) > 0:\n",
    "        vec_ = np.divide(vec_, len(word_list))\n",
    "    return vec_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg1 = avg_vec(text1, model)\n",
    "avg2 = avg_vec(text2, model)\n",
    "avg3 = avg_vec(text3, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "前回と同じ方法でコサイン距離を求めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コサイン類似度: txt1 vs txt2 :[[0.5231217]]\n",
      "コサイン類似度: txt1 vs txt3 :[[0.46388638]]\n",
      "コサイン類似度: txt2 vs txt3 :[[0.3673437]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim1 = cosine_similarity(avg1.reshape(1, -1), avg2.reshape(1, -1))\n",
    "print (f'コサイン類似度: txt1 vs txt2 :{cos_sim1}')\n",
    "cos_sim2 = cosine_similarity(avg1.reshape(1, -1), avg3.reshape(1, -1))\n",
    "print (f'コサイン類似度: txt1 vs txt3 :{cos_sim2}')\n",
    "cos_sim3 = cosine_similarity(avg2.reshape(1, -1), avg3.reshape(1, -1))\n",
    "print (f'コサイン類似度: txt2 vs txt3 :{cos_sim3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[エッフェル塔]', 0.5456256866455078),\n",
       " ('マドリッド', 0.5358479022979736),\n",
       " ('[シャンゼリゼ通り]', 0.49942439794540405),\n",
       " ('[トラファルガー広場]', 0.49436208605766296),\n",
       " ('[リール_(フランス)]', 0.49335700273513794),\n",
       " ('ヴィラ', 0.4929209351539612),\n",
       " ('[エトワール凱旋門]', 0.49175509810447693),\n",
       " ('[パレ・ロワイヤル]', 0.49152642488479614),\n",
       " ('ナポリ', 0.49030327796936035),\n",
       " ('シャトー', 0.488348126411438)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## スカイツリ - 日本 + フランス\n",
    "model.most_similar(positive=['スカイツリー', 'フランス'], negative=['日本'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[日本茶]', 0.6305805444717407),\n",
       " ('[泡盛]', 0.6159665584564209),\n",
       " ('[日本酒]', 0.5956202149391174),\n",
       " ('[清酒]', 0.5938064455986023),\n",
       " ('焼酎', 0.5927814841270447),\n",
       " ('[焼酎]', 0.5906597375869751),\n",
       " ('清酒', 0.5873808860778809),\n",
       " ('地酒', 0.5791105031967163),\n",
       " ('日本酒', 0.5710263848304749),\n",
       " ('[納豆]', 0.5674401521682739)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ワイン - フランス + 日本 \n",
    "model.most_similar(positive=['ワイン', '日本'], negative=['フランス'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "こうした単語分散表現は他にも公開されています。\n",
    "「白ヤギコーポレーション」[^siroyagi] モデルは、やはり日本語ウィキペディアから学習されています。\n",
    "[^siroyagi]: <https://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model_path = 'word2vec.gensim.model'\n",
    "model = Word2Vec.load(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "name": "Chapter11_wordvectors.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
