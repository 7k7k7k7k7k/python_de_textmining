{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  huggingface-transformers(BERT)\n",
    "\n",
    "本書の最後に、最近の自然言語処理技術を利用するためのフレームワークである Transformer を紹介します。また、Transformer で利用する言語モデルとして BERT を取り上げます。\n",
    "\n",
    "## ディープラーニングと自然言語処理\n",
    "\n",
    "\n",
    "以下、本書の最後に、huggingface-transoformers （ BERT）を利用したテキスト処理の技法を紹介しましょう（GiNZA を紹介した章で述べたように、GiNZA で Transformer にもとづく学習モデルを読み込むという方法もあります）。\n",
    "\n",
    "ところで、ディープラーニングは計算量が多いため、標準的なパソコンでは処理に非常に時間がかかることがあります。そこでパソコンにグラフィックボードという画像処理のハードウェア(**GPU**)を追加することで、処理の多くをGPUに分散させ、負荷の軽減と高速化をはかることができます。\n",
    "\n",
    "\n",
    "とはいえ、GPUの導入とドライバのインストールは簡単ではありません。\n",
    "幸い、自身のパソコンにGPUがない場合でも、 huggingface-transformers を試す方法があります。Google Collabaratory [^gcollab] という無償のWEBサービスを使うことです（以下 Colab と表記します）。Colab は、ここまで利用してきた Jupyter (Jupyter Labo) とほぼ同じ感覚で利用することができます。\n",
    "\n",
    "\n",
    "Google Coraboratory での作業方法については、巻末の付録の付録にも記していますが、ここでも改めて解説いたします。\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "## Google Colaboratory における Mecabのインストール\n",
    "!apt install mecab libmecab-dev mecab-ipadic-utf8\n",
    "!pip install mecab-python3\n",
    "!pip install fugashi ipadic\n",
    "```\n",
    "\n",
    "\n",
    "`!pip install transformers==4.12.0`\n",
    "\n",
    "なお、Colabではなく自身のマシンで実行する場合は torch (PyTorch) もインストールしてください。\n",
    "筆者が Colab 上で作業したファイルを Google Colab に公開していますので、その URL をサポートサイトで確認してください。\n",
    "\n",
    "<https://colab.research.google.com/drive/1E13hvgiCmh_eZtvnnZHh59s3_FTR4I9J>\n",
    "\n",
    "\n",
    "このノートブックを自身のドライブにコピーした上で、試してみて下さい。\n",
    "\n",
    "\n",
    "## transformers によるトークン化\n",
    "\n",
    "transformers で日本語を扱うには、日本語トークンにもとづく事前学習モデルを導入する必要があります。\n",
    "\n",
    "transformers では東北大学の自然言語処理研究室が開発したモデルを利用することできます。このモデルは、日本語ウキペディアをデータとして学習されたモデルになります。ここでは'bert-base-japanese-whole-word-masking'を利用しますが、2021年により大きなモデル 'bert-large-japanese'が公開されています。ただし、大きなモデルを使う場合、GPU のメモリが足りず、RuntimeError: CUDA error: out of memory というエラーで作業が進まなくなることがあるので、注意してください。\n",
    "\n",
    "なお `AutoTokenizer` モジュールは、指定されたモデルのトークナイザーに適切な設定を行ってくれます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  5233,     5,  1757,  1882,  2762,     5,  5770,     9, 14872,\n",
      "          422,  1581,    75,     8,     3])\n",
      "['[CLS]', '最近', 'の', '自然', '言語', '処理', 'の', '主流', 'は', 'ディープ', '##ラー', '##ニング', 'だ', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "## ここでは'bert-base-japanese-whole-word-masking'を利用しますが、2021年により大きなモデル 'bert-large-japanese'が公開されています。\n",
    "## ただし、大きなモデルを使う場合、GPU のメモリが足りず、RuntimeError: CUDA error: out of memory \n",
    "## というエラーで作業が進まなくなることがあるので、注意してください。\n",
    "japanese_model = ('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "# \n",
    "tokenizer = AutoTokenizer.from_pretrained(japanese_model)\n",
    "res = tokenizer.tokenize('最近の自然言語処理の主流はディープラーニングだ。')\n",
    "# print(res)\n",
    "ids = tokenizer.convert_tokens_to_ids(res)\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "## 単語IDを確認\n",
    "print(ids)\n",
    "## 対応するトークン（形態素、文字など）を確認\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 単語ID\n",
    "\n",
    "トークン化について説明しましたが、単語はそのままで処理されるわけではありません。内部でトークンには一意のID（番号）が割り当てられ、それが入力となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 306, 9, 26724, 11, 2949, 10, 8, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('彼は蕎麦を食べた。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークン穴埋め問題\n",
    "\n",
    "\n",
    "transformers を使って文章をトークンに分割できるようになりました。次に、分割した結果を言語モデルに適用してみましょう。\n",
    "transformers では、空白を推定するタスクを行うのに **AutoModelForMaskedLM** クラスに、言語ごとに用意されたモデルをアタッチします。以前は言語モデルごとにクラス名が異なってました。\n",
    "たとえば、BERT 日本語モデルを指定するという意味で、BertForMaskedLM クラスで日本語モデルを読み込んでいました。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "masked_model = AutoModelForMaskedLM.from_pretrained(japanese_model)\n",
    "## GPUを搭載しているのであれば、GPUのメモリを使う\n",
    "masked_model = masked_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今日', 'は', '[MASK]', 'で', '勉強', 'し', 'た', '。']\n"
     ]
    }
   ],
   "source": [
    "text = '今日は[MASK]で勉強した。'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "トークン列を符号化して、モデルへの入力とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "encoded_text = encoded_text.cuda()\n",
    "with torch.no_grad():\n",
    "    output = masked_model(input_ids=encoded_text)\n",
    "    scores = output.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力の `scores` は 3 次元の配列になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socresのサイズ：torch.Size([1, 10, 32000])\n"
     ]
    }
   ],
   "source": [
    "print(f'socresのサイズ：{scores.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各トークンのID：[2, 3246, 9, 4, 12, 8192, 15, 10, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "print(f'各トークンのID：{encoded_text[0].tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID＝396\n",
      "トークン=大学\n"
     ]
    }
   ],
   "source": [
    "mask_position = encoded_text[0].tolist().index(4)\n",
    "best_id = scores[0, mask_position].argmax(-1).item()\n",
    "print(f'ID＝{best_id}')\n",
    "best_token = tokenizer.convert_ids_to_tokens(best_id)\n",
    "print(f'トークン={best_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  396,  1411,  1724,   286, 18949,  1221,  4441,   723,  2184,  1193],\n",
      "       device='cuda:0')\n",
      "['大学', 'ここ', 'ニューヨーク', 'アメリカ', 'コロンビア大学', 'そこ', 'ロサンゼルス', 'イギリス', 'パリ', '高校']\n"
     ]
    }
   ],
   "source": [
    "topK = scores[0, mask_position].topk(10)\n",
    "print(topK.indices)\n",
    "tokens  = tokenizer.convert_ids_to_tokens(topK.indices)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline\n",
    "\n",
    "huggingface-transformers には自然言語処理でよく行われる処理について、学修済みモデルを簡単に適用できる **pipeline** という仕組みがあります[^pipeline]。以下に一例をあげます。\n",
    "\n",
    "\n",
    "- 文章穴埋め ('fill-mask')\n",
    "- 感情分析 ('sentiment-analysis')\n",
    "- テキスト分類 ('text-classification')\n",
    "- 固有表現抽出 ('ner')\n",
    "- 質問応答 ('question-answeri')\n",
    "- 文章要約 ('summarization')\n",
    "- 翻訳 ('translation')\n",
    "\n",
    "[^pipeline]: https://huggingface.co/docs/transformers/main_classes/pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': '今日 は 大学 で 勉強 し た 。', 'score': 0.06624536216259003, 'token': 396, 'token_str': '大 学'}, {'sequence': '今日 は ここ で 勉強 し た 。', 'score': 0.03463226929306984, 'token': 1411, 'token_str': 'こ こ'}, {'sequence': '今日 は ニューヨーク で 勉強 し た 。', 'score': 0.032337456941604614, 'token': 1724, 'token_str': 'ニ ュ ー ヨ ー ク'}, {'sequence': '今日 は アメリカ で 勉強 し た 。', 'score': 0.027580933645367622, 'token': 286, 'token_str': 'ア メ リ カ'}, {'sequence': '今日 は コロンビア大学 で 勉強 し た 。', 'score': 0.022066786885261536, 'token': 18949, 'token_str': 'コ ロ ン ビ ア 大 学'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model=japanese_model, tokenizer=tokenizer)\n",
    "print(unmasker('今日は[MASK]で勉強した。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## huggingface transformers によるテキスト分類\n",
    "\n",
    "このタスクのクラス名は **AutoModelForSequenceClassification** となります。\n",
    "ここでジャンル判定の応用として、文章の内容がネガティブなのかポジティブなのか判定する **センチメント分析** を試してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'ネガティブ', 'score': 0.7547961473464966}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, pipeline\n",
    "## 日本語感情分析用のモデルをロードする\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained ('daigo/bert-base-japanese-sentiment')\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=sentiment_model, tokenizer=tokenizer)\n",
    "print(sentiment_analyzer('ロシアとウクライナの戦争はまだ終わらない。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ここではテキストをジャンルごとに分類を行うための学習済みモデルを、新たに用意したテキストデータセットでファインチューニングする方法を紹介しましょう\n",
    "\n",
    "\n",
    "\n",
    "まず分析対象とするテキストデータセットを用意します。ここでは、自然言語処理でベンチマークとしてよく利用される Livedoor ニュースコーパスを利用させてもらいます[^livedoor]。\n",
    "株式会社ロンウイットのサイトから ldcc-20140209.tar.gz というファイルをダウンロードします。\n",
    "\n",
    "[^livedoor]: https://www.rondhuit.com/download.html#ldcc\n",
    "\n",
    "以下では Python の関数を使ってダウンロードと解凍を行っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## データセットのダウンロード\n",
    "# !wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "# !tar xvzf ldcc-20140209.tar.gz\n",
    "from urllib import request\n",
    "request.urlretrieve(\"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\", \"ldcc-20140209.tar.gz\")\n",
    "## 解凍\n",
    "import tarfile\n",
    "with tarfile.open('ldcc-20140209.tar.gz', 'r:gz') as t:\n",
    "    t.extractall(path='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお、Google Colaboratory を利用している場合は、保存用のフォルダ（ディレクトリ）を用意します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google Colaboratory で作業する場合\n",
    "from google.colab import drive \n",
    "drive.mount('/content/drive')\n",
    "!mkdir -p /content/drive/MyDrive\n",
    "## 作業フォルダを移動\n",
    "%cd /content/drive/MyDrive\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/2bddf92b-47f9-4809-95a5-b91e7f25af27/myData/GitHub/textmining_python/textmining/docs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作業フォルダ text には 10 個のサブフォルダが含まれています。サブフォルダ名を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smax', 'kaden-channel', 'it-life-hack', 'dokujo-tsushin', 'livedoor-homme', 'sports-watch', 'movie-enter', 'peachy', 'text', 'topic-news']\n"
     ]
    }
   ],
   "source": [
    "## サブフォルダを確認\n",
    "categories = [name for name in os.listdir(\"../livedoor/text\") if os.path.isdir(\"../livedoor/text/\" + name)]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "categories = [name for name in os.listdir(\"text\") if os.path.isdir(\"text/\" + name)]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Livedoor ファイルには 10 種類のジャンルのファイルがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>以前は株式や国債などの市場動向をチェックするには証券会社の店頭に足を運ぶか、専門誌や専門テレ...</td>\n",
       "      <td>it-life-hack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Androidは端末が将来OSのアップデートに対応するかどうかは、最新の環境で使いたい場合に...</td>\n",
       "      <td>it-life-hack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>先日の記事「ある日突然犯罪者扱いに？　無許可ダウンロードの罰則化が引き起こす問題」で動きがあ...</td>\n",
       "      <td>it-life-hack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ソフトバンクグループの代表 孫正義氏は、Twitterを通じて活発な発言をしている。そんな同...</td>\n",
       "      <td>it-life-hack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wordでは、縦スクロールバーの下にジャンプボタンが用意されている。ポンと押すだけで次のペー...</td>\n",
       "      <td>it-life-hack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences        labels\n",
       "0  以前は株式や国債などの市場動向をチェックするには証券会社の店頭に足を運ぶか、専門誌や専門テレ...  it-life-hack\n",
       "1  Androidは端末が将来OSのアップデートに対応するかどうかは、最新の環境で使いたい場合に...  it-life-hack\n",
       "2  先日の記事「ある日突然犯罪者扱いに？　無許可ダウンロードの罰則化が引き起こす問題」で動きがあ...  it-life-hack\n",
       "3  ソフトバンクグループの代表 孫正義氏は、Twitterを通じて活発な発言をしている。そんな同...  it-life-hack\n",
       "4  Wordでは、縦スクロールバーの下にジャンプボタンが用意されている。ポンと押すだけで次のペー...  it-life-hack"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "categories = ['it-life-hack', 'dokujo-tsushin'] \n",
    "datasets = pd.DataFrame(columns=[\"sentences\", \"labels\"])\n",
    "for label, cat in enumerate(categories):\n",
    "    for file in glob(f'../livedoor/text/{cat}/{cat}*'):\n",
    "        ## Google Colaboratory の場合は file in glob(f'text/{cat}/{cat}*'): と変更\n",
    "        lines = open(file).read().splitlines()\n",
    "        body = '\\n'.join(lines[3:])\n",
    "        sentences = pd.Series([body, cat], index=datasets.columns)\n",
    "        datasets = datasets.append(sentences, ignore_index=True)\n",
    "\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストのジャンルを表す文字列を数値に変えます。it-life-hack には 0 を、dokujo-tsushin には 1 を対応させます。この対応を辞書として用意し `map()` で lables 列に一括適用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it-life-hack': 0, 'dokujo-tsushin': 1}\n"
     ]
    }
   ],
   "source": [
    "cat_id = dict(zip(categories, list(range(len(categories)))))\n",
    "print(cat_id)\n",
    "datasets['labels'] = datasets['labels'].map(cat_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "1735    1\n",
      "1736    1\n",
      "1737    1\n",
      "1738    1\n",
      "1739    1\n",
      "Name: labels, Length: 1740, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(datasets['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、transformers を使ってトークンに分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "japanese_model = 'cl-tohoku/bert-base-japanese-whole-word-masking'# 'cl-tohoku/bert-large-japanese' # '\n",
    "tokenizer = AutoTokenizer.from_pretrained(japanese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで用意したデータフレームを、訓練用とテスト用に分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "## ラベル別にindexを取得\n",
    "label0 = datasets.query('labels==0').index\n",
    "label1 = datasets.query('labels==1').index\n",
    "## それぞれから500行を取り出して訓練データとする\n",
    "rnd0 = random.sample(list(label0), 500)\n",
    "rnd1 = random.sample(list(label1), 500)\n",
    "idx = rnd0 + rnd1\n",
    "train_data = datasets.iloc[idx]\n",
    "## 残りをテストデータとする\n",
    "test_data = datasets.drop(index=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>ターガスと言えば、PC関連、特にノートPCを収納しつつ機能性に富むビジネスバッグの定番と言え...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>販促イベントや催事、展示即売会、運動会や体育祭、文化祭、音楽祭といった行事で統一感を出したい...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentences  labels\n",
       "864  ターガスと言えば、PC関連、特にノートPCを収納しつつ機能性に富むビジネスバッグの定番と言え...       0\n",
       "394  販促イベントや催事、展示即売会、運動会や体育祭、文化祭、音楽祭といった行事で統一感を出したい...       0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 冒頭の2行を確認\n",
    "train_data.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、それぞれのデータを huggingface transformers の Trainer クラスに適用できるように加工します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_encodings = tokenizer(train_data['sentences'].tolist(),\n",
    "                            return_tensors='pt',\n",
    "                            padding=True, truncation=True,\n",
    "                            max_length=128).to(device)\n",
    "test_encodings = tokenizer(test_data['sentences'].tolist(),\n",
    "                           return_tensors='pt',\n",
    "                           padding=True, truncation=True,\n",
    "                           max_length=128).to(device)\n",
    "train_labels = torch.tensor(train_data['labels'].tolist())\n",
    "test_labels =  torch.tensor(test_data['labels'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、これを **Dataset** というクラスのオブジェクトに変換します。このために、クラスを独自に定義します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveDoor_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "## 実際にデータを変換する\n",
    "train_dataset = LiveDoor_Dataset(train_encodings, train_labels)\n",
    "test_dataset = LiveDoor_Dataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json from cache at /home/ishida/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/pytorch_model.bin from cache at /home/ishida/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(    \n",
    "    japanese_model,\n",
    "    num_labels = 2, \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False\n",
    ")\n",
    "\n",
    "## モデルをGPUに載せる\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainingArguments クラスに学習の精度を評価するメソッドを指定するために定義をしておきます。\n",
    "**評価指標** とは、機械学習やディープラーニングにおいては、予測値あるいは分類の精度を検討するための基準のことです。\n",
    "\n",
    "- 正解率 (Accuracy)\n",
    "- 精度 (Precision)\n",
    "- 検出率 (Recall)\n",
    "- F 値 (F-measure, F-score, F1 Score)\n",
    "\n",
    "\n",
    "これらを説明するには **混同行列** を知っておく必要があります。\n",
    "\n",
    "|メール番号 | スパムか否か|予測結果 |\n",
    "|-------|---------|--------|\n",
    "|メール1 |0 | 0|\n",
    "|メール2 |0 | 1|\n",
    "|メール3 |0 | 0|\n",
    "|メール4 |0 | 0| \n",
    "|メール5 |1 | 1 |\n",
    "|メール6 |0 | 0 |\n",
    "|メール7 |1 | 1|\n",
    "|メール8 |1 | 1 |\n",
    "|メール9 |0 | 0 |\n",
    "|メール10 |1 | 1|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "true_label = [0, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n",
    "pred_label = [0, 0, 0, 1, 0, 1, 1, 1, 0, 1]\n",
    "cm = confusion_matrix(true_label, pred_label)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的には以下のような表になります。\n",
    "\n",
    "\n",
    "|        |分類結果        |             |\n",
    "|        |　スパム(1)と分類 | スパム(0)と分類　|\n",
    "|実際     |　            |            　|\n",
    "|--------|-------------|--------------|\n",
    "|スパム(1) |      4      |       2     |\n",
    "|非スパム(0)|      1      |      3       |\n",
    "\n",
    "\n",
    "さて、混同行列では、各セルが次の評価に対応します。\n",
    "\n",
    "\n",
    "\n",
    "|     |分類結果|    |\n",
    "|     |　陽性    | 陰性　         |\n",
    "|実際  |　       |    　         |\n",
    "|-----|--------|--------------|\n",
    "| 陽性 | TP 真陽性 |   FN 真陰性|\n",
    "| 陰性 | FP 偽陽性 |  TF 真陰性 |\n",
    "\n",
    "\n",
    "- TP (True-Positive) 真陽性： 本当は陽性（スパム）であるメールを、正しく陽性と判定\n",
    "- TN (True-Negative) 真陰性：本当は陰性（非スパム）を、正しく陰性と判定\n",
    "- FP (False-Positive) 偽陽性： 本当は陰性であるメールを、誤って陽性と判定\n",
    "- FN (False-Negative) 偽陰性： 本当は陽性であるメールを、誤って陰性と判定\n",
    "\n",
    "\n",
    "\n",
    "それぞれの個数は次のように求められます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "tp, fn, fp, tn = cm.ravel()\n",
    "print((tp, fn, fp, tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率: 0.7\n",
      "適合率: 0.6\n",
      "感度: 0.75\n",
      "F 値: 0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f'正解率: {accuracy_score(true_label, pred_label)}')\n",
    "print(f'適合率: {precision_score(true_label, pred_label)}')\n",
    "print(f'感度: {recall_score(true_label, pred_label)}')\n",
    "print(f'F 値: {f1_score(true_label, pred_label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお、正解率、適応率、F値を一度に求められる `precision_recall_fscore_support` というメソッドもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "## 4 つの指標を計算する関数を定義\n",
    "def cal_4metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データまた評価指標の用意ができたので、Trainer クラスを使って学習を行います\n",
    "ここでは、パソコンにあまり負荷をかけず、早期に学習が終了することを優先した設定としています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 63\n",
      "<ipython-input-111-08016269cb46>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/63 : < :, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 740\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=0.5820352766248915, metrics={'train_runtime': 21.4777, 'train_samples_per_second': 46.56, 'train_steps_per_second': 2.933, 'total_flos': 65777763840000.0, 'train_loss': 0.5820352766248915, 'epoch': 1.0})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    num_train_epochs=1, \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=64, \n",
    "    warmup_steps=500,  \n",
    "    weight_decay=0.01, \n",
    "    save_total_limit=1, \n",
    "    dataloader_pin_memory=False, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset, \n",
    "    compute_metrics=cal_4metrics \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取り除けておいたテストデータを評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 740\n",
      "  Batch size = 64\n",
      "<ipython-input-111-08016269cb46>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/12 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33966121077537537, 'eval_accuracy': 0.9283783783783783, 'eval_f1': 0.9283772012323065, 'eval_precision': 0.9284065424315696, 'eval_recall': 0.9283783783783783, 'eval_runtime': 3.7505, 'eval_samples_per_second': 197.306, 'eval_steps_per_second': 3.2, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(trainer.evaluate(eval_dataset=test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のテストデータの評価結果を見ると、accuracy が 0.92 F1 score が 0.92 となりました。\n",
    "\n",
    "ファインチューニングしたモデルは以下のように保存することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./LiveDoor_model/tokenizer_config.json\n",
      "Special tokens file saved in ./LiveDoor_model/special_tokens_map.json\n",
      "Configuration saved in ./LiveDoor_model/config.json\n",
      "Model weights saved in ./LiveDoor_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "model_directory = './LiveDoor_model'\n",
    "tokenizer.save_pretrained(model_directory)\n",
    "model.save_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存したモデルを読み込む場合には以下のようにします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./LiveDoor_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./LiveDoor_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./LiveDoor_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_directory = './LiveDoor_model'\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(model_directory)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "以上、 huggingface-transformers による自然言語処理の実行例を示しました。\n",
    "最初にも述べたように、ディープラーニングに基づくライブラリは更新が早く、現在のバージョンでは動作したコードであっても、しばらく後には期待通りの出力が得られないということが多々あります。\n",
    "そのため、利用するつど最新バージョンにおける関数の定義などを確認する必要があります。\n",
    "\n",
    "その一方で、ディープラーニングに基づく最新の自然言語処理技術を反映した huggingface-transformers はしばらくの間、デファクトスタンダードの地位を維持すると予想されます。\n",
    "関数などの仕様の変更は続くと思われますが、その考え方や処理の流れが大きく変化することは当面ないかもしれません。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "name": "Chapter12_transformer.ipynb",
  "vscode": {
   "interpreter": {
    "hash": "8c1cc9189a8cb268605973668f4dacfb44b0c3a77a6199cbbf771dd310aadae0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
